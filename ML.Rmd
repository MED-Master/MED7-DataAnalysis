---
title: "ML"
author: "HZ"
date: "08/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(skimr)
library(stringr)
library(corrplot)
library(RColorBrewer)
library("PerformanceAnalytics")
library("Hmisc")
library(Rmisc)
library(lubridate)
library(ggbeeswarm)
library(GGally)
library(effsize)
library(magrittr)
library(dplyr)
library(reticulate)
library(TTR)
library(corrplot)
library(Hmisc)
library(caret)
library(data.table)
library(viridis)
library(hrbrthemes)
library(psycho)
```

```{python Loading packages, include=FALSE}
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None 
import matplotlib.pyplot as plt
from scipy import stats as sc
import sklearn as sk
import sklearn.preprocessing as pr
import sklearn.decomposition as decomp
import biosppy
import matplotlib
import nolds
import hrvanalysis as hrv

import lazypredict
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesRegressor
from lazypredict.Supervised import LazyRegressor
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

import seaborn as sns
# seaborn settings
sns.set_style("whitegrid")
sns.set_context("talk")

from sklearn.preprocessing import LabelEncoder

# Import necessary modules
from math import sqrt
from sklearn.metrics import classification_report,r2_score
from sklearn.svm import SVC
```

```{r load in data, include=FALSE}
data <- read.csv("processed_data.csv")
```

```{python standardization, include=FALSE}
py_data = r.data
py_data1 = py_data.drop(['file_path', 'Timestamp', 'condition'], axis=1)

scaler = pr.StandardScaler()
scaled = scaler.fit(py_data1)
scaled = pd.DataFrame(scaler.transform(py_data1), columns = py_data1.columns)

scaled["file_path"] = py_data["file_path"]
scaled["Timestamp"] = py_data["Timestamp"]
scaled["condition"] = py_data["condition"]


scaler = pr.MinMaxScaler()
normalised = scaler.fit(py_data1)
normalised = pd.DataFrame(scaler.transform(py_data1), columns = py_data1.columns)

normalised["file_path"] = py_data["file_path"]
normalised["Timestamp"] = py_data["Timestamp"]
normalised["condition"] = py_data["condition"]
```

```{r save data into r, include=FALSE}
raw_data <- py$py_data 
scaled_data <- py$scaled
normalised_data <- py$normalised

raw_data_VE <- raw_data %>% filter(condition == "VE")
raw_data_NOVE <- raw_data %>% filter(condition == "No VE")
raw_data_nohrv <- raw_data[, 13:ncol(raw_data)]

scaled_data_nohrv <- scaled_data[, 13:ncol(scaled_data)] 
scaled_data_VE <- data.frame(scaled_data) %>% dplyr::filter(condition == "VE") 
scaled_data_NOVE <- data.frame(scaled_data) %>%  dplyr::filter(condition == "No VE")
scaled_data_VENOHRV <- scaled_data_VE[, 13:ncol(scaled_data_VE)]
scaled_data_NOVENOHRV <- scaled_data_NOVE[, 13:ncol(scaled_data_NOVE)]
scaled_phys  <- scaled_data[, 1:12]
scaled_phys$file_path <- scaled_data$file_path
scaled_phys$Timestamp <- scaled_data$Timestamp
scaled_phys$condition <- scaled_data$condition

normalised_data_nohrv <- normalised_data[, 13:ncol(normalised_data)] 
normalised_data_VE <- data.frame(normalised_data) %>%  dplyr::filter(condition == "VE") 
normalised_data_NOVE <- data.frame(normalised_data) %>%  dplyr::filter(condition == "No VE")
normalised_data_VENOHRV <- normalised_data_VE[, 13:ncol(normalised_data_VE)]

normalised_data_VEphys <- normalised_data_VE[, 1:12]
normalised_data_VEphys$file_path <- normalised_data_VE$file_path
normalised_data_VEphys$Timestamp <- normalised_data_VE$Timestamp
normalised_data_VEphys$condition <- normalised_data_VE$condition

normalised_data_NOVEphys <- normalised_data_NOVE[, 1:12]
normalised_data_NOVEphys$file_path <- normalised_data_NOVE$file_path
normalised_data_NOVEphys$Timestamp <- normalised_data_NOVE$Timestamp
normalised_data_NOVEphys$condition <- normalised_data_NOVE$condition

normalised_data_NOVENOHRV <- normalised_data_NOVE[, 13:ncol(normalised_data_NOVE)]
normalised_phys  <- normalised_data[, 1:12]
normalised_phys$file_path <- normalised_data$file_path
normalised_phys$Timestamp <- normalised_data$Timestamp
normalised_phys$condition <- normalised_data$condition
```

```{python PCA}
def pcaanlysis(df):
  df = df.set_index("Timestamp")
  df = df.drop(["file_path", "condition"], axis=1)
  pca = decomp.PCA(n_components = 0.90)
  df_pca = pca.fit_transform(df)
  test = pd.DataFrame(pca.components_, columns = df.columns)
  n_pcs= pca.n_components_
  res = pd.DataFrame(pca.components_, columns = df.columns)
  return res


py_scaled = pd.DataFrame(r.scaled_data)
py_scaled.name = "py_scaled"
pca_py_scaled = pcaanlysis(py_scaled)
#################################################################
py_scaled_nohrv = pd.DataFrame(r.scaled_data_nohrv)
py_scaled_nohrv.name = "py_scaled_nohrv"
pca_py_scaled_nohrv = pcaanlysis(py_scaled_nohrv)

#################################################################
py_scaled_data_VE = pd.DataFrame(r.scaled_data_VE)
py_scaled_data_VE.name = "py_scaled_data_VE"
print(pcaanlysis(py_scaled_data_VE))
#################################################################
py_scaled_data_NOVE = pd.DataFrame(r.normalised_data_NOVE)
py_scaled_data_NOVE.name = "py_scaled_data_NOVE"
print(pcaanlysis(py_scaled_data_NOVE))
#################################################################
py_scaled_data_VENOHRV = pd.DataFrame(r.scaled_data_VENOHRV)
py_scaled_data_VENOHRV.name = "py_scaled_data_VENOHRV"
print(pcaanlysis(py_scaled_data_VENOHRV))
#################################################################
py_scaled_data_NOVENOHRV = pd.DataFrame(r.scaled_data_NOVENOHRV)
py_scaled_data_NOVENOHRV.name = "py_scaled_data_NOVENOHRV"
print(pcaanlysis(py_scaled_data_NOVENOHRV))
#################################################################
py_scaled_phys = pd.DataFrame(r.scaled_phys)
py_scaled_phys.name = "py_scaled_phys"
pca_py_scaled_phys = pcaanlysis(py_scaled_phys)
#################################################################

py_norm = pd.DataFrame(r.normalised_data)
py_norm.name = "py_norm"
pca_py_norm = pcaanlysis(py_norm)
#################################################################
py_norm_nohrv = pd.DataFrame(r.normalised_data_nohrv)
py_norm_nohrv.name = "py_norm_nohrv"
pca_py_norm_nohrv = pcaanlysis(py_norm_nohrv)
pca_py_norm_nohrv.name = "py_norm_nohrv"
#################################################################
py_normal_data_VE = pd.DataFrame(r.normalised_data_VE)
py_normal_data_VE.name = "py_normal_data_VE"
print(pcaanlysis(py_normal_data_VE))
#################################################################
py_normal_data_NOVE = pd.DataFrame(r.normalised_data_NOVE)
py_normal_data_NOVE.name = "py_normal_data_NOVE"
print(pcaanlysis(py_normal_data_NOVE))
#################################################################
py_normalised_data_VENOHRV = pd.DataFrame(r.normalised_data_VENOHRV)
py_normalised_data_VENOHRV.name = "py_normalised_data_VENOHRV"
print(pcaanlysis(py_normalised_data_VENOHRV))
#################################################################
py_normalised_data_NOVENOHRV = pd.DataFrame(r.normalised_data_NOVENOHRV)
py_normalised_data_NOVENOHRV.name = "py_normalised_data_NOVENOHRV"
print(pcaanlysis(py_normalised_data_NOVENOHRV))
#################################################################
py_normalised_phys = pd.DataFrame(r.normalised_phys)
py_normalised_phys.name = "py_normalised_phys"
pca_py_normalised_phys = pcaanlysis(py_normalised_phys)
#################################################################
py_normalised_data_VEphys = pd.DataFrame(r.normalised_data_VEphys)
py_normalised_data_VEphys.name = "py_normalised_data_VEphys"
#################################################################
py_normalised_data_NOVEphys = pd.DataFrame(r.normalised_data_NOVEphys)
py_normalised_data_NOVEphys.name = "py_normalised_data_NOVEphys"


for s in range(1, 22):
  isolated = py_scaled[py_scaled["file_path"] == s]
  print(isolated)
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_scaled_nohrv[py_scaled_nohrv["file_path"] == s]
  print(isolated)
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  
  
for s in range(1, 22):
  isolated = py_scaled_phys[py_scaled_phys["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  
  
for s in range(1, 22):
  isolated = py_norm[py_norm["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_norm_nohrv[py_norm_nohrv["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_normalised_phys[py_normalised_phys["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")


```


```{python regression}
def regress(df_data, df_physio, xname, yname):
  x = df_data.drop(["file_path", "condition", "Timestamp"], axis=1)
  y = df_physio[yname]
  
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)
  reg = LazyRegressor(predictions=True)
  models, predictions = reg.fit(X_train, X_test, y_train, y_test)
  
  models.to_csv(xname + "_" + yname + ".csv")
  
  return models

namearray = ["IBI", "max_hr", "min_hr", "rmssd", "sdnn", "lf", "hf", "lf_hf_ratio", "lfnu", "hfnu", "EDA", "SmoothedEDA"]

for i in namearray:
  regress(py_scaled_nohrv, py_scaled_phys, py_scaled_nohrv.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_norm_nohrv, py_normalised_phys, py_norm_nohrv.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_normalised_data_VENOHRV, py_normalised_data_VEphys, py_normalised_data_VENOHRV.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_normalised_data_NOVENOHRV, py_normalised_data_NOVEphys, py_normalised_data_NOVENOHRV.name, i)
  print(i)
  
  
  
filtered_data = py_norm_nohrv[(py_norm_nohrv["file_path"] != 15) & (py_norm_nohrv["file_path"] != 16)]
filtered_data.name = "filtered"
filtered_phys = py_normalised_phys[(py_normalised_phys["file_path"] != 15) & (py_normalised_phys["file_path"] != 16)]
for i in namearray:
  regress(filtered_data, filtered_phys, filtered_data.name, i)
  print(i)
  
  
for s in range(1, 22):
  isolated_NOHRV = py_norm_nohrv[py_norm_nohrv["file_path"] == s]
  isolated_NOHRV.name = str(s)
  isolated_phys = py_normalised_phys[py_normalised_phys["file_path"] == s]
  for i in namearray:
    regress(isolated_NOHRV, isolated_phys, isolated_NOHRV.name, i)
    print("###################################################")
  
for i in namearray:
  regress(py_norm_nohrv, py_normalised_phys, py_norm_nohrv.name, i)
  print(i)
  

```

```{python regression}
def et(xdf, ydf, name):
  x = xdf.drop(["file_path", "condition", "Timestamp"], axis=1)
  y = ydf[name]
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)
  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
  tes = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(X_train, y_train)
  print(tes.score(X_test, y_test))
  imp = pd.DataFrame(x.columns)
  imp["importance"] = tes.feature_importances_
  print(imp)


et(py_scaled_nohrv, py_scaled_phys, "SmoothedEDA")

filtered_data = py_norm_nohrv[(py_norm_nohrv["file_path"] != 15) & (py_norm_nohrv["file_path"] != 16)]
filtered_data.name = "filtered"
filtered_phys = py_normalised_phys[(py_normalised_phys["file_path"] != 15) & (py_normalised_phys["file_path"] != 16)]
et(filtered_data, filtered_phys, "IBI")
et(filtered_data, filtered_phys, "max_hr")
et(filtered_data, filtered_phys, "min_hr")
et(filtered_data, filtered_phys, "rmssd")
et(filtered_data, filtered_phys, "hf")
```

```{python regression on stroke patient from healthy people data}
def etstroke(xdf, ydf, name):
  x = xdf.drop(["file_path", "condition", "Timestamp"], axis=1)
  x_test = x[(x["file_path"] == 21)]
  x = x[(x["file_path"] != 21)]
  
  y = ydf[name]
  y_test = [(y["file_path"] == 21)]
  y = y[(y["file_path"] != 21)]
  
  tes = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(x, y)
  print(tes.score(X_test, y_test))
  imp = pd.DataFrame(x.columns)
  imp["importance"] = tes.feature_importances_
  print(imp)


et(py_scaled_nohrv, py_scaled_phys, "rmssd")
```

```{python PCA with figure}
def pcaanlysiswithfig(df):
  #df = df.set_index("Timestamp")
  #df = df.drop(["file_path", "condition"], axis=1)
  pca = decomp.PCA(n_components = 0.8)
  df_pca = pd.DataFrame(pca.fit_transform(df))
  n_components = len(pca.explained_variance_ratio_)
  explained_variance = pca.explained_variance_ratio_
  cum_explained_variance = np.cumsum(explained_variance)
  idx = np.arange(n_components)+1
  df_explained_variance = pd.DataFrame([explained_variance, cum_explained_variance], index=['explained variance', 'cumulative'], columns=idx).T
  mean_explained_variance = df_explained_variance.iloc[:,0].mean() # calculate mean explained variance
# (iii.) Print explained variance as plain text
  print('PCA Overview')
  print('='*40)
  print("Total: {} components".format(n_components))
  print('-'*40)
  print('Mean explained variance:', round(mean_explained_variance,3))
  print('-'*40)
  print(df_explained_variance.head(20))
  print('-'*40)
  most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_components)]
  initial_feature_names = df.columns
  most_important_names = [initial_feature_names[most_important[i]] for i in range(n_components)]
  #df_pca = pd.DataFrame(pca.components_[i])
  #df_pca["variance %"] = pca.explained_variance_ratio_*100
  #print(df_pca)
  print(most_important_names)
  principalDf = pd.DataFrame(data = df_pca , columns = ['principal component 1', 'principal component 2', 'principal component 2'])
  print(principalDf)
  plt.clf()
  #df = df[[most_important_names]].columns
  cmap = sns.diverging_palette(150,150, as_cmap=True, l=85)
  ax = sns.heatmap(pca.components_,cmap=cmap,yticklabels=[ "PCA"+str(x) for x in range(1,pca.n_components_+1)],xticklabels=list(df.columns), annot=True, annot_kws={"fontsize":14})
  plt.xticks(rotation=45, rotation_mode='anchor', ha='right') 
  ax.figure.savefig('hmx.png', transparent=True, bbox_inches='tight')
  #ax.figure.subplots_adjust(left = 0.3)
  plt.show(ax)
  return 0

test = pd.DataFrame(py_scaled_nohrv[['righthanddist_mean', 'Hits_sum', 'mean_diameter_3d', 'Shots_mean', 'headrot_sum', 'headdist_max', 'max_diameter']])
dict = {'righthanddist_mean': 'Mean Hand Movement',
        'Hits_sum': 'Mean Hits',
        'mean_diameter_3d': 'Mean Eye Diameter',
        'Shots_mean': 'Mean shots Taken',
        'headrot_sum': 'Total head Rotation',
        'headdist_max': 'Maximum head Movement',
        'max_diameter': 'Maximum Pupil Diameter'}
test.rename(columns=dict, inplace=True)
pca_py_scaled_nohrv = pcaanlysiswithfig(test)
```

```{python Classification2}
df = py_data[py_data["file_path"] <= 18]
df2 = py_data[py_data["file_path"] > 18]
target_column = ['condition']
df = df.drop(["file_path", "Timestamp"], axis=1)
predictors = list(set(list(df.columns))-set(target_column))
df[predictors] = df[predictors]/df[predictors].max()
X = df[predictors].values
y = df[target_column].values
df2 = df2.drop(["file_path", "Timestamp"], axis=1)
predictors2 = list(set(list(df2.columns))-set(target_column))
df2[predictors2] = df2[predictors2]/df[predictors2].max()
X2 = df2[predictors2].values
y2 = df2[target_column].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)
mlp = SVC(gamma='auto', kernel = "linear")
mlp.fit(X_train,y_train)
predict_train = mlp.predict(X_train)
predict_test = mlp.predict(X_test)
print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))
predict_sec = mlp.predict(X2)
print(confusion_matrix(y2,predict_sec))
print(classification_report(y2,predict_sec))
print(pd.DataFrame(mlp.coef_))

df = py_data
target_column = ['condition']
df = df.drop(["file_path", "Timestamp"], axis=1)
predictors = list(set(list(df.columns))-set(target_column))
df[predictors] = df[predictors]/df[predictors].max()
X = df[predictors].values
y = df[target_column].values
mlp = SVC(gamma='auto', kernel = "linear")
scores = cross_val_score(mlp, X, y, cv=10)
print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))
print(pd.DataFrame(mlp.coef_))
```

```{r splitting and agg for svm, include=FALSE}
tail_raw <- data  %>% group_by(file_path) %>% slice_tail(n = 60)
tail_raw$condition[(tail_raw$file_path %% 2) == 0] <- "VE"
tail_raw$condition[(tail_raw$file_path %% 2) != 0] <- "NO VE"
tail_raw$condition[(tail_raw$file_path == 21)] <- "VE"

tail_raw_pc <- tail_raw %>% select(righthanddist_mean, condition, max_diameter, mean_diameter_3d, headrot_sum,headdist_max, -file_path)
tail_raw_pc <- tail_raw_pc %>% ungroup()
tail_raw_pc <- tail_raw_pc %>% select(-file_path)

tail_agg <- tail_raw %>% group_by(file_path) %>% summarise_at(vars(-condition, -Timestamp), funs(mean(., na.rm=TRUE)))
tail_agg$condition[(tail_agg$file_path %% 2) == 0] <- "VE"
tail_agg$condition[(tail_agg$file_path %% 2) != 0] <- "NO VE"
tail_agg$condition[(tail_agg$file_path == 21)] <- "VE"

tail_agg_nophys <- tail_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
tail_agg_nophys$condition = 10
tail_raw_nophys <- tail_raw %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)

scaled_phys_tail <- scaled_phys%>% group_by(file_path) %>% slice_tail(n = 60)
scaled_phys_tail <- scaled_phys_tail %>% filter(file_path != 15, file_path != 16, file_path != 15, file_path != 2)
scaled_phys_tail<- scaled_phys_tail %>% group_by(file_path) %>% summarise_at(vars(-condition), funs(mean(., na.rm=TRUE)))
scaled_phys_tail$condition[(scaled_phys_tail$file_path %% 2) == 0] <- "VE"
scaled_phys_tail$condition[(scaled_phys_tail$file_path %% 2) != 0] <- "NO VE"
scaled_phys_tail$condition[(scaled_phys_tail$file_path == 21)] <- "VE"



head_raw <- data  %>% group_by(file_path) %>% slice_head(n = 60)
head_raw$condition[(head_raw$file_path %% 2) == 0] <- "VE"
head_raw$condition[(head_raw$file_path %% 2) != 0] <- "NO VE"
head_raw$condition[(head_raw$file_path == 21)] <- "VE"

head_agg <- head_raw %>% group_by(file_path) %>% summarise_at(vars(-condition, -Timestamp), funs(mean(., na.rm=TRUE)))
head_agg$condition[(head_agg$file_path %% 2) == 0] <- "VE"
head_agg$condition[(head_agg$file_path %% 2) != 0] <- "NO VE"
head_agg$condition[(head_agg$file_path == 21)] <- "VE"

head_agg_nophys <- head_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
head_agg_nophys$condition= 1

scaled_phys_head <- scaled_phys%>% group_by(file_path) %>% slice_head(n = 60)
scaled_phys_head <- scaled_phys_head %>% filter(file_path != 15, file_path != 16, file_path != 15, file_path != 2)
scaled_phys_head<- scaled_phys_head %>% group_by(file_path) %>% summarise_at(vars(-condition), funs(mean(., na.rm=TRUE)))
scaled_phys_head$condition[(scaled_phys_head$file_path %% 2) == 0] <- "VE"
scaled_phys_head$condition[(scaled_phys_head$file_path %% 2) != 0] <- "NO VE"
scaled_phys_head$condition[(scaled_phys_head$file_path == 21)] <- "VE"

tracking_con <- rbind(tail_agg_nophys, head_agg_nophys)
tracking_con$condition <-0

bind_head_phys <- scaled_phys_head
bind_head_phys$condition <- "1"
bind_tail_phys <- scaled_phys_tail
bind_tail_phys$condition <- "10"
phys_minute = rbind(bind_tail_phys, bind_head_phys)

ve_head_phys <- scaled_phys_head %>% filter(condition == "VE")
ve_head_phys$condition <- "1"
ve_tail_phys <- scaled_phys_tail %>% filter(condition == "VE")
ve_tail_phys$condition <- "10"
phys_minute_ve <- rbind(ve_head_phys, ve_tail_phys)

nove_head_phys <- scaled_phys_head %>% filter(condition == "NO VE")
nove_head_phys$condition <- "1"
nove_tail_phys <- scaled_phys_tail %>% filter(condition == "NO VE")
nove_tail_phys$condition <- "10"
phys_minute_nove <- rbind(nove_head_phys, nove_tail_phys)

agg_nophys = rbind(tail_agg_nophys, head_agg_nophys)


ve_tail_agg_nophys <- tail_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
ve_tail_agg_nophys <- ve_tail_agg_nophys %>% filter(condition == "VE")
ve_tail_agg_nophys$condition = 10
ve_head_agg_nophys <- head_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
ve_head_agg_nophys <- ve_head_agg_nophys %>% filter(condition == "VE")
ve_head_agg_nophys$condition = 1
ve_agg_nophys = rbind(ve_head_agg_nophys, ve_tail_agg_nophys)

nove_tail_agg_nophys <- tail_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
nove_tail_agg_nophys <- nove_tail_agg_nophys %>% filter(condition == "NO VE")
nove_tail_agg_nophys$condition = 10
nove_head_agg_nophys <- head_agg %>% dplyr::select(-lf, -hf, -lfnu, -hfnu, -sdnn, -rmssd, -IBI, -EDA, -SmoothedEDA, -min_hr, -max_hr, -lf_hf_ratio)
nove_head_agg_nophys <- nove_tail_agg_nophys %>% filter(condition == "NO VE")
nove_head_agg_nophys$condition = 1
nove_agg_nophys = rbind(nove_head_agg_nophys, nove_tail_agg_nophys)
```

```{python Classification2}
df = r.ve_agg_nophys
target_column = ['condition']
df = df.drop(["file_path"], axis=1)
predictors = list(set(list(df.columns))-set(target_column))
df[predictors] = df[predictors]/df[predictors].max()
X = df[predictors].values
y = df[target_column].values
mlp = SVC(gamma='auto', kernel = "linear")
scores = cross_val_score(mlp, X, y, cv=8)
scores
scores.mean()
scores.std()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)
mlp = SVC(gamma='auto', kernel = "linear")
mlp.fit(X_train,y_train)
predict_train = mlp.predict(X_train)
predict_test = mlp.predict(X_test)

print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))
results = pd.DataFrame(mlp.coef_).T
results["feat"] = df[predictors].columns
results
```