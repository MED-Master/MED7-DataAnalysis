---
title: "ML"
author: "HZ"
date: "08/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(skimr)
library(stringr)
library(corrplot)
library(RColorBrewer)
library("PerformanceAnalytics")
library("Hmisc")
library(Rmisc)
library(lubridate)
library(ggbeeswarm)
library(GGally)
library(effsize)
library(magrittr)
library(dplyr)
library(reticulate)
library(TTR)
library(corrplot)
library(Hmisc)
library(caret)
library(data.table)
library(viridis)
library(hrbrthemes)
library(psycho)
```

```{python Loading packages, include=FALSE}
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None 
import matplotlib.pyplot as plt
from scipy import stats as sc
import sklearn as sk
import sklearn.preprocessing as pr
import sklearn.decomposition as decomp
import biosppy
import matplotlib
import nolds
import hrvanalysis as hrv

import lazypredict
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesRegressor
from lazypredict.Supervised import LazyRegressor
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

import seaborn as sns
# seaborn settings
sns.set_style("whitegrid")
sns.set_context("talk")
```

```{r load in data, include=FALSE}
data <- read.csv("processed_data.csv")
```

```{python standardization, include=FALSE}
py_data = r.data

py_data1 = py_data.drop(['file_path', 'Timestamp', 'condition'], axis=1)

scaler = pr.StandardScaler()
scaled = scaler.fit(py_data1)
scaled = pd.DataFrame(scaler.transform(py_data1), columns = py_data1.columns)

scaled["file_path"] = py_data["file_path"]
scaled["Timestamp"] = py_data["Timestamp"]
scaled["condition"] = py_data["condition"]


scaler = pr.MinMaxScaler()
normalised = scaler.fit(py_data1)
normalised = pd.DataFrame(scaler.transform(py_data1), columns = py_data1.columns)

normalised["file_path"] = py_data["file_path"]
normalised["Timestamp"] = py_data["Timestamp"]
normalised["condition"] = py_data["condition"]
```

```{r save data into r, include=FALSE}
raw_data <- py$py_data 
scaled_data <- py$scaled
normalised_data <- py$normalised

raw_data_VE <- raw_data %>% filter(condition == "VE")
raw_data_NOVE <- raw_data %>% filter(condition == "No VE")
raw_data_nohrv <- raw_data[, 13:ncol(raw_data)]

scaled_data_nohrv <- scaled_data[, 13:ncol(scaled_data)] 
scaled_data_VE <- data.frame(scaled_data) %>% dplyr::filter(condition == "VE") 
scaled_data_NOVE <- data.frame(scaled_data) %>%  dplyr::filter(condition == "No VE")
scaled_data_VENOHRV <- scaled_data_VE[, 13:ncol(scaled_data_VE)]
scaled_data_NOVENOHRV <- scaled_data_NOVE[, 13:ncol(scaled_data_NOVE)]
scaled_phys  <- scaled_data[, 1:12]
scaled_phys$file_path <- scaled_data$file_path
scaled_phys$Timestamp <- scaled_data$Timestamp
scaled_phys$condition <- scaled_data$condition

normalised_data_nohrv <- normalised_data[, 13:ncol(normalised_data)] 
normalised_data_VE <- data.frame(normalised_data) %>%  dplyr::filter(condition == "VE") 
normalised_data_NOVE <- data.frame(normalised_data) %>%  dplyr::filter(condition == "No VE")
normalised_data_VENOHRV <- normalised_data_VE[, 13:ncol(normalised_data_VE)]

normalised_data_VEphys <- normalised_data_VE[, 1:12]
normalised_data_VEphys$file_path <- normalised_data_VE$file_path
normalised_data_VEphys$Timestamp <- normalised_data_VE$Timestamp
normalised_data_VEphys$condition <- normalised_data_VE$condition

normalised_data_NOVEphys <- normalised_data_NOVE[, 1:12]
normalised_data_NOVEphys$file_path <- normalised_data_NOVE$file_path
normalised_data_NOVEphys$Timestamp <- normalised_data_NOVE$Timestamp
normalised_data_NOVEphys$condition <- normalised_data_NOVE$condition

normalised_data_NOVENOHRV <- normalised_data_NOVE[, 13:ncol(normalised_data_NOVE)]
normalised_phys  <- normalised_data[, 1:12]
normalised_phys$file_path <- normalised_data$file_path
normalised_phys$Timestamp <- normalised_data$Timestamp
normalised_phys$condition <- normalised_data$condition
```

```{python PCA}
def pcaanlysis(df):
  df = df.set_index("Timestamp")
  df = df.drop(["file_path", "condition"], axis=1)
  pca = decomp.PCA(n_components = 0.90)
  df_pca = pca.fit_transform(df)
  test = pd.DataFrame(pca.components_, columns = df.columns)
  n_pcs= pca.n_components_
  most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]
  initial_feature_names = df.columns
  most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]
  df_pca = pd.DataFrame(pca.components_[i])
  df_pca["variance %"] = pca.explained_variance_ratio_*100
  print(df_pca)
  res = pd.DataFrame(pca.components_, columns = df.columns)
  return res


py_scaled = pd.DataFrame(r.scaled_data)
py_scaled.name = "py_scaled"
pca_py_scaled = pcaanlysis(py_scaled)
#################################################################
py_scaled_nohrv = pd.DataFrame(r.scaled_data_nohrv)
py_scaled_nohrv.name = "py_scaled_nohrv"
pca_py_scaled_nohrv = pcaanlysis(py_scaled_nohrv)

#################################################################
py_scaled_data_VE = pd.DataFrame(r.scaled_data_VE)
py_scaled_data_VE.name = "py_scaled_data_VE"
print(pcaanlysis(py_scaled_data_VE))
#################################################################
py_scaled_data_NOVE = pd.DataFrame(r.normalised_data_NOVE)
py_scaled_data_NOVE.name = "py_scaled_data_NOVE"
print(pcaanlysis(py_scaled_data_NOVE))
#################################################################
py_scaled_data_VENOHRV = pd.DataFrame(r.scaled_data_VENOHRV)
py_scaled_data_VENOHRV.name = "py_scaled_data_VENOHRV"
print(pcaanlysis(py_scaled_data_VENOHRV))
#################################################################
py_scaled_data_NOVENOHRV = pd.DataFrame(r.scaled_data_NOVENOHRV)
py_scaled_data_NOVENOHRV.name = "py_scaled_data_NOVENOHRV"
print(pcaanlysis(py_scaled_data_NOVENOHRV))
#################################################################
py_scaled_phys = pd.DataFrame(r.scaled_phys)
py_scaled_phys.name = "py_scaled_phys"
pca_py_scaled_phys = pcaanlysis(py_scaled_phys)
#################################################################

py_norm = pd.DataFrame(r.normalised_data)
py_norm.name = "py_norm"
pca_py_norm = pcaanlysis(py_norm)
#################################################################
py_norm_nohrv = pd.DataFrame(r.normalised_data_nohrv)
py_norm_nohrv.name = "py_norm_nohrv"
pca_py_norm_nohrv = pcaanlysis(py_norm_nohrv)
pca_py_norm_nohrv.name = "py_norm_nohrv"
#################################################################
py_normal_data_VE = pd.DataFrame(r.normalised_data_VE)
py_normal_data_VE.name = "py_normal_data_VE"
print(pcaanlysis(py_normal_data_VE))
#################################################################
py_normal_data_NOVE = pd.DataFrame(r.normalised_data_NOVE)
py_normal_data_NOVE.name = "py_normal_data_NOVE"
print(pcaanlysis(py_normal_data_NOVE))
#################################################################
py_normalised_data_VENOHRV = pd.DataFrame(r.normalised_data_VENOHRV)
py_normalised_data_VENOHRV.name = "py_normalised_data_VENOHRV"
print(pcaanlysis(py_normalised_data_VENOHRV))
#################################################################
py_normalised_data_NOVENOHRV = pd.DataFrame(r.normalised_data_NOVENOHRV)
py_normalised_data_NOVENOHRV.name = "py_normalised_data_NOVENOHRV"
print(pcaanlysis(py_normalised_data_NOVENOHRV))
#################################################################
py_normalised_phys = pd.DataFrame(r.normalised_phys)
py_normalised_phys.name = "py_normalised_phys"
pca_py_normalised_phys = pcaanlysis(py_normalised_phys)
#################################################################
py_normalised_data_VEphys = pd.DataFrame(r.normalised_data_VEphys)
py_normalised_data_VEphys.name = "py_normalised_data_VEphys"
#################################################################
py_normalised_data_NOVEphys = pd.DataFrame(r.normalised_data_NOVEphys)
py_normalised_data_NOVEphys.name = "py_normalised_data_NOVEphys"


for s in range(1, 22):
  isolated = py_scaled[py_scaled["file_path"] == s]
  print(isolated)
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_scaled_nohrv[py_scaled_nohrv["file_path"] == s]
  print(isolated)
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  
  
for s in range(1, 22):
  isolated = py_scaled_phys[py_scaled_phys["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  
  
for s in range(1, 22):
  isolated = py_norm[py_norm["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_norm_nohrv[py_norm_nohrv["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")
  

for s in range(1, 22):
  isolated = py_normalised_phys[py_normalised_phys["file_path"] == s]
  print(s)
  print(pcaanlysis(isolated))
  print("###################################################")


```


```{python regression}
def regress(df_data, df_physio, xname, yname):
  x = df_data.drop(["file_path", "condition", "Timestamp"], axis=1)
  y = df_physio[yname]
  
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)
  reg = LazyRegressor(predictions=True)
  models, predictions = reg.fit(X_train, X_test, y_train, y_test)
  
  models.to_csv(xname + "_" + yname + ".csv")
  
  return models

namearray = ["IBI", "max_hr", "min_hr", "rmssd", "sdnn", "lf", "hf", "lf_hf_ratio", "lfnu", "hfnu", "EDA", "SmoothedEDA"]

for i in namearray:
  regress(py_scaled_nohrv, py_scaled_phys, py_scaled_nohrv.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_norm_nohrv, py_normalised_phys, py_norm_nohrv.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_normalised_data_VENOHRV, py_normalised_data_VEphys, py_normalised_data_VENOHRV.name, i)
  print(i)
  
  
for i in namearray:
  regress(py_normalised_data_NOVENOHRV, py_normalised_data_NOVEphys, py_normalised_data_NOVENOHRV.name, i)
  print(i)
  
  
  
filtered_data = py_norm_nohrv[(py_norm_nohrv["file_path"] != 15) & (py_norm_nohrv["file_path"] != 16)]
filtered_data.name = "filtered"
filtered_phys = py_normalised_phys[(py_normalised_phys["file_path"] != 15) & (py_normalised_phys["file_path"] != 16)]
for i in namearray:
  regress(filtered_data, filtered_phys, filtered_data.name, i)
  print(i)
  
  
for s in range(1, 22):
  isolated_NOHRV = py_norm_nohrv[py_norm_nohrv["file_path"] == s]
  isolated_NOHRV.name = str(s)
  isolated_phys = py_normalised_phys[py_normalised_phys["file_path"] == s]
  for i in namearray:
    regress(isolated_NOHRV, isolated_phys, isolated_NOHRV.name, i)
    print("###################################################")
  
for i in namearray:
  regress(py_norm_nohrv, py_normalised_phys, py_norm_nohrv.name, i)
  print(i)
  

```

```{python regression}
def et(xdf, ydf, name):
  x = xdf.drop(["file_path", "condition", "Timestamp"], axis=1)
  y = ydf[name]
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)
  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
  tes = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(X_train, y_train)
  print(tes.score(X_test, y_test))
  imp = pd.DataFrame(x.columns)
  imp["importance"] = tes.feature_importances_
  print(imp)


et(py_scaled_nohrv, py_scaled_phys, "SmoothedEDA")

filtered_data = py_norm_nohrv[(py_norm_nohrv["file_path"] != 15) & (py_norm_nohrv["file_path"] != 16)]
filtered_data.name = "filtered"
filtered_phys = py_normalised_phys[(py_normalised_phys["file_path"] != 15) & (py_normalised_phys["file_path"] != 16)]
et(filtered_data, filtered_phys, "IBI")
et(filtered_data, filtered_phys, "max_hr")
et(filtered_data, filtered_phys, "min_hr")
et(filtered_data, filtered_phys, "rmssd")
et(filtered_data, filtered_phys, "hf")
```

```{python regression on stroke patient from healthy people data}
def etstroke(xdf, ydf, name):
  x = xdf.drop(["file_path", "condition", "Timestamp"], axis=1)
  x_test = x[(x["file_path"] == 21)]
  x = x[(x["file_path"] != 21)]
  
  y = ydf[name]
  y_test = [(y["file_path"] == 21)]
  y = y[(y["file_path"] != 21)]
  
  tes = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(x, y)
  print(tes.score(X_test, y_test))
  imp = pd.DataFrame(x.columns)
  imp["importance"] = tes.feature_importances_
  print(imp)


et(py_scaled_nohrv, py_scaled_phys, "rmssd")
```

```{python PCA with figure}
def pcaanlysiswithfig(df):
  #df = df.set_index("Timestamp")
  #df = df.drop(["file_path", "condition"], axis=1)
  pca = decomp.PCA(n_components = 0.8)
  df_pca = pd.DataFrame(pca.fit_transform(df))
  n_components = len(pca.explained_variance_ratio_)
  explained_variance = pca.explained_variance_ratio_
  cum_explained_variance = np.cumsum(explained_variance)
  idx = np.arange(n_components)+1
  df_explained_variance = pd.DataFrame([explained_variance, cum_explained_variance], index=['explained variance', 'cumulative'], columns=idx).T
  mean_explained_variance = df_explained_variance.iloc[:,0].mean() # calculate mean explained variance
# (iii.) Print explained variance as plain text
  print('PCA Overview')
  print('='*40)
  print("Total: {} components".format(n_components))
  print('-'*40)
  print('Mean explained variance:', round(mean_explained_variance,3))
  print('-'*40)
  print(df_explained_variance.head(20))
  print('-'*40)
  most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_components)]
  initial_feature_names = df.columns
  most_important_names = [initial_feature_names[most_important[i]] for i in range(n_components)]
  #df_pca = pd.DataFrame(pca.components_[i])
  #df_pca["variance %"] = pca.explained_variance_ratio_*100
  #print(df_pca)
  print(most_important_names)
  principalDf = pd.DataFrame(data = df_pca , columns = ['principal component 1', 'principal component 2', 'principal component 2'])
  print(principalDf)
  plt.clf()
  #df = df[[most_important_names]].columns
  ax = sns.heatmap(pca.components_,cmap='YlGnBu',yticklabels=[ "PCA"+str(x) for x in range(1,pca.n_components_+1)],xticklabels=list(df.columns), annot=True, annot_kws={"fontsize":14})
  plt.xticks(rotation=45, rotation_mode='anchor', ha='right') 
  ax.figure.savefig('hmx.png', transparent=True, bbox_inches='tight')
  #ax.figure.subplots_adjust(left = 0.3)
  plt.show(ax)
  return 0

test = pd.DataFrame(py_scaled_nohrv[['righthanddist_mean', 'Hits_sum', 'mean_diameter_3d', 'Shots_mean', 'headrot_sum', 'headdist_max', 'max_diameter']])
dict = {'righthanddist_mean': 'Mean Hand Movement',
        'Hits_sum': 'Mean Hits',
        'mean_diameter_3d': 'Mean Eye Diameter',
        'Shots_mean': 'Mean shots Taken',
        'headrot_sum': 'Total head Rotation',
        'headdist_max': 'Maximum head Movement',
        'max_diameter': 'Maximum Pupil Diameter'}
test.rename(columns=dict, inplace=True)
pca_py_scaled_nohrv = pcaanlysiswithfig(test)
```